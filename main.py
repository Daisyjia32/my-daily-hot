import requests
import os
import json
from datetime import datetime
from urllib.parse import quote

FEISHU_WEBHOOK_URL = os.environ['FEISHU_WEBHOOK_URL']

def get_weibo_hot():
    """æŠ“å–å¾®åšçƒ­æœæ¦œ"""
    try:
        url = "https://weibo.com/ajax/side/hotSearch"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://weibo.com/',
        }
        response = requests.get(url, headers=headers, timeout=10)
        print(f"å¾®åšæ¥å£çŠ¶æ€ç : {response.status_code}")
        if response.status_code == 200:
            data = response.json()
            hot_list = []
            count = 0
            
            for item in data['data']['realtime']:
                if item.get('realpos') and item.get('realpos', 0) > 0 and item.get('flag') != 2:
                    search_word = quote(item['word'])
                    weibo_url = f"https://s.weibo.com/weibo?q={search_word}"
                    
                    hot_list.append({
                        'title': item['note'],
                        'url': weibo_url,
                        'rank': item['realpos']
                    })
                    count += 1
                    if count >= 10:
                        break
            
            hot_list.sort(key=lambda x: x['rank'])
            print(f"è¿‡æ»¤åçš„å¾®åšçƒ­æœæ•°é‡: {len(hot_list)}")
            return hot_list
        else:
            print(f"å¾®åšè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")
            return []
    except Exception as e:
        print(f"è·å–å¾®åšçƒ­æœå‡ºé”™: {e}")
        return []

def get_zhihu_hot():
    """æŠ“å–çŸ¥ä¹çƒ­æ¦œ"""
    try:
        url = "https://api.zhihu.com/topstory/hot-list?limit=10"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://www.zhihu.com/',
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        print(f"çŸ¥ä¹æ¥å£çŠ¶æ€ç : {response.status_code}")
        
        if response.status_code == 200:
            data = response.json()
            hot_list = []
            for item in data['data']:
                target = item.get('target', {})
                title = target.get('title', 'æ— æ ‡é¢˜')
                url = target.get('url', '')
                
                # ä¿®å¤çŸ¥ä¹é“¾æ¥
                if url and 'api.zhihu.com' in url:
                    if '/questions/' in url:
                        question_id = url.split('/questions/')[-1]
                        url = f"https://www.zhihu.com/question/{question_id}"
                    else:
                        url = "https://www.zhihu.com/hot"
                elif url and 'zhihu.com' not in url:
                    url = f"https://www.zhihu.com/question/{target.get('id', '')}"
                elif not url:
                    url = "https://www.zhihu.com/hot"
                
                hot_list.append({
                    'title': title,
                    'url': url
                })
            return hot_list
        else:
            print("çŸ¥ä¹æ¥å£å¤±è´¥ï¼Œè¿”å›æç¤ºä¿¡æ¯")
            return [{
                'title': 'âš ï¸ çŸ¥ä¹çƒ­æ¦œæš‚æ—¶æ— æ³•è·å–',
                'url': 'https://www.zhihu.com/hot'
            }]
            
    except Exception as e:
        print(f"è·å–çŸ¥ä¹çƒ­æ¦œå‡ºé”™: {e}")
        return [{
            'title': 'âš ï¸ çŸ¥ä¹çƒ­æ¦œè·å–å‡ºé”™',
            'url': 'https://www.zhihu.com/hot'
        }]

def get_newrank_low_fans():
    """æŠ“å–æ–°æ¦œä½ç²‰çˆ†æ–‡æ¦œTOP10 - ç‚¹å‡»è·å–çœŸå®é“¾æ¥ç‰ˆ"""
    try:
        from playwright.sync_api import sync_playwright
        import os
        import re
        
        print("å¼€å§‹æŠ“å–æ–°æ¦œä½ç²‰çˆ†æ–‡æ¦œ...")
        newrank_list = []
        
        # ä»ç¯å¢ƒå˜é‡è·å–Cookie
        newrank_cookie = os.environ.get('NEWRANK_COOKIE', '')
        
        if not newrank_cookie:
            return [{
                'title': 'âš ï¸ æœªè®¾ç½®æ–°æ¦œCookie',
                'url': 'https://www.newrank.cn/hotInfo?platform=GZH&rankType=3'
            }]
        
        print("ä½¿ç”¨Playwrightç‚¹å‡»è·å–çœŸå®æ–‡ç« é“¾æ¥...")
        
        with sync_playwright() as p:
            # å¯åŠ¨æµè§ˆå™¨
            browser = p.chromium.launch(headless=True)
            context = browser.new_context()
            page = context.new_page()
            
            # è®¾ç½®Cookie
            print("è®¾ç½®ç™»å½•Cookie...")
            cookies = []
            cookie_pairs = newrank_cookie.split(';')
            
            for cookie_str in cookie_pairs:
                cookie_str = cookie_str.strip()
                if '=' in cookie_str:
                    name, value = cookie_str.split('=', 1)
                    cookies.append({
                        'name': name.strip(),
                        'value': value.strip(),
                        'domain': '.newrank.cn',
                        'path': '/'
                    })
            
            # å…ˆè®¿é—®é¦–é¡µè®¾ç½®Cookie
            page.goto('https://www.newrank.cn/', timeout=30000)
            context.add_cookies(cookies)
            
            # è®¿é—®ç›®æ ‡é¡µé¢
            print("è®¿é—®ä½ç²‰çˆ†æ–‡æ¦œé¡µé¢...")
            page.goto('https://www.newrank.cn/hotInfo?platform=GZH&rankType=3', timeout=60000)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            print("ç­‰å¾…æ¦œå•æ•°æ®åŠ è½½...")
            page.wait_for_timeout(8000)
            
            # ç­–ç•¥ï¼šç›´æ¥æŸ¥æ‰¾æ¦œå•ä¸­çš„æ–‡ç« æ ‡é¢˜å¹¶ç‚¹å‡»è·å–çœŸå®é“¾æ¥
            print("æŸ¥æ‰¾æ¦œå•ä¸­çš„æ–‡ç« æ ‡é¢˜å…ƒç´ ...")
            
            # ä¿å­˜å½“å‰é¡µé¢çš„URLï¼ˆç”¨äºè¿”å›ï¼‰
            original_url = page.url
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ…å«æ–‡ç« æ ‡é¢˜çš„å…ƒç´ 
            all_elements = page.query_selector_all('*')
            potential_titles = []
            
            for element in all_elements:
                try:
                    text = element.inner_text().strip()
                    # åŒ¹é…æ–‡ç« æ ‡é¢˜ç‰¹å¾ï¼ˆåŸºäºé™ˆé“æ˜é‚£æ¡çš„ç‰¹å¾ï¼‰
                    if (len(text) > 15 and len(text) < 100 and
                        any(char in text for char in ['ï¼š', 'ï¼', 'ï¼Œ', 'ã€‚', 'ï¼Ÿ', 'â€¦']) and
                        not any(keyword in text for keyword in [  # æ’é™¤éæ–‡ç« æ–‡æœ¬
                            'ç™»å½•', 'æ³¨å†Œ', 'é¦–é¡µ', 'æ–°æ¦œ', 'è½»æ¾', 'è´¦å·', 'æ‰¾å·', 'ç¤¾åª’',
                            'è¥é”€', 'æ¨å¹¿', 'æŠ•æ”¾', 'åˆ›ä½œ', 'æ•°æ®', 'å›é‡‡', 'ä½œå“', 'æä¾›',
                            'åŠ©åŠ›', 'å¢å€¼', 'ç‚¹å‡»', 'å‰å¾€', 'æ­£å¼è¿è¥', 'åŠå…¬å®¤'
                        ]) and
                        not re.match(r'^[0-9\.\s]*$', text)):  # æ’é™¤çº¯æ•°å­—å’Œç‚¹
                        
                        potential_titles.append({
                            'text': text,
                            'element': element
                        })
                except:
                    continue
            
            print(f"æ‰¾åˆ° {len(potential_titles)} ä¸ªå¯èƒ½çš„æ–‡ç« æ ‡é¢˜")
            
            # æ˜¾ç¤ºå‰10ä¸ªç”¨äºè°ƒè¯•
            for i, title in enumerate(potential_titles[:10]):
                print(f"æ ‡é¢˜ {i+1}: {title['text']}")
            
            # å°è¯•ç‚¹å‡»è¿™äº›æ ‡é¢˜æ¥è·å–çœŸå®é“¾æ¥
            count = 0
            seen_titles = set()
            
            for title_info in potential_titles:
                if count >= 10:
                    break
                
                title = title_info['text']
                element = title_info['element']
                
                if title in seen_titles:
                    continue
                seen_titles.add(title)
                
                try:
                    # ç‚¹å‡»æ ‡é¢˜å…ƒç´ 
                    print(f"å°è¯•ç‚¹å‡»: {title}")
                    
                    # åœ¨æ–°æ ‡ç­¾é¡µä¸­æ‰“å¼€é“¾æ¥
                    with page.expect_popup() as popup_info:
                        element.click(button="middle")  # ä¸­é”®ç‚¹å‡»åœ¨æ–°æ ‡ç­¾é¡µæ‰“å¼€
                    
                    popup = popup_info.value
                    popup.wait_for_load_state()
                    
                    # è·å–æ–°æ ‡ç­¾é¡µçš„URL
                    popup_url = popup.url
                    print(f"ç‚¹å‡»åè·³è½¬åˆ°: {popup_url}")
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å¾®ä¿¡æ–‡ç« é“¾æ¥
                    if 'mp.weixin.qq.com' in popup_url:
                        # è¿™æ˜¯çœŸæ­£çš„å¾®ä¿¡æ–‡ç« é“¾æ¥ï¼
                        newrank_list.append({
                            'title': title,
                            'url': popup_url
                        })
                        count += 1
                        print(f"âœ… æˆåŠŸè·å–ç¬¬{count}æ¡: {title}")
                        print(f"   çœŸå®é“¾æ¥: {popup_url}")
                    
                    # å…³é—­æ–°æ ‡ç­¾é¡µ
                    popup.close()
                    
                    # ç­‰å¾…ä¸€ä¸‹å†å¤„ç†ä¸‹ä¸€ä¸ª
                    page.wait_for_timeout(1000)
                    
                except Exception as e:
                    print(f"ç‚¹å‡»å¤±è´¥: {e}")
                    continue
            
            # å¦‚æœç‚¹å‡»æ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥è§£æé¡µé¢ç»“æ„
            if not newrank_list:
                print("ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥è§£æé¡µé¢ç»“æ„...")
                
                # æŸ¥æ‰¾åŒ…å«æ’åå’Œæ–‡ç« ä¿¡æ¯çš„ç‰¹å®šç»“æ„
                page_text = page.inner_text('body')
                
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ’åå’Œæ ‡é¢˜
                # åŒ¹é…æ¨¡å¼ï¼šæ•°å­—. æ ‡é¢˜ï¼ˆç›´åˆ°ç²‰ä¸æ•°æˆ–é˜…è¯»æ•°ï¼‰
                pattern = r'(\d+)\.\s*([^â€¦]+?â€¦|[^â€¦]+?)(?=\s+[^\s]+\s+ç²‰ä¸æ•°|\s+[0-9Ww\+]+\s*æ”¶è—|\s*$)'
                matches = re.findall(pattern, page_text)
                
                print(f"æ­£åˆ™åŒ¹é…åˆ° {len(matches)} ä¸ªæ–‡ç« æ ‡é¢˜")
                
                for rank, title in matches[:10]:
                    title = title.strip()
                    if len(title) > 5 and title not in seen_titles:
                        seen_titles.add(title)
                        
                        # å¯¹äºåŒ¹é…åˆ°çš„æ ‡é¢˜ï¼Œæˆ‘ä»¬åªèƒ½æä¾›æ–°æ¦œé¡µé¢é“¾æ¥
                        # å› ä¸ºæ— æ³•é€šè¿‡ç®€å•æ–¹æ³•è·å–å¾®ä¿¡æ–‡ç« é“¾æ¥
                        newrank_list.append({
                            'title': title,
                            'url': original_url  # ä½¿ç”¨åŸå§‹é¡µé¢é“¾æ¥
                        })
                        count += 1
                        print(f"âœ… å¤‡ç”¨æ–¹æ¡ˆç¬¬{count}æ¡: {title}")
            
            browser.close()
        
        print(f"æˆåŠŸè·å–æ–°æ¦œæ•°æ® {len(newrank_list)} æ¡")
        
        if not newrank_list:
            return [{
                'title': 'âš ï¸ æ— æ³•è·å–æ–‡ç« æ•°æ®',
                'url': 'https://www.newrank.cn/hotInfo?platform=GZH&rankType=3'
            }]
        
        return newrank_list
        
    except Exception as e:
        print(f"è·å–æ–°æ¦œä½ç²‰çˆ†æ–‡æ¦œå‡ºé”™: {e}")
        return [{
            'title': 'âš ï¸ æ–°æ¦œä½ç²‰çˆ†æ–‡æ¦œè·å–å¤±è´¥',
            'url': 'https://www.newrank.cn/hotInfo?platform=GZH&rankType=3'
        }]
        
def send_to_feishu(weibo_data, zhihu_data, newrank_data):
    """å‘é€æ¶ˆæ¯åˆ°é£ä¹¦"""
    text_content = "ğŸŒ æ¯æ—¥çƒ­ç‚¹é€Ÿé€’\n\n"
    
    # å¾®åšéƒ¨åˆ†
    if weibo_data and len(weibo_data) > 0:
        text_content += "ã€ğŸ”¥ å¾®åšçƒ­æœ TOP 10ã€‘\n"
        for i, item in enumerate(weibo_data, 1):
            text_content += f"{i}. {item['title']}\n   ğŸ”— {item['url']}\n"
        text_content += "\n"
    
    # çŸ¥ä¹éƒ¨åˆ†
    if zhihu_data and len(zhihu_data) > 0:
        text_content += "ã€ğŸ“š çŸ¥ä¹çƒ­æ¦œ TOP 30ã€‘\n"
        for i, item in enumerate(zhihu_data, 1):
            text_content += f"{i}. {item['title']}\n"
            if 'zhihu.com' in item['url']:
                text_content += f"   ğŸ”— {item['url']}\n"
        text_content += "\n"
    
    # æ–°æ¦œä½ç²‰çˆ†æ–‡æ¦œéƒ¨åˆ†
    if newrank_data and len(newrank_data) > 0:
        text_content += "ã€ğŸ’¥ æ–°æ¦œä½ç²‰çˆ†æ–‡æ¦œ TOP 10ã€‘\n"
        for i, item in enumerate(newrank_data, 1):
            text_content += f"{i}. {item['title']}\n"
            if 'newrank.cn' in item['url']:
                text_content += f"   ğŸ”— {item['url']}\n"
        text_content += "\n"
    
    # æ·»åŠ æ—¶é—´æˆ³
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    text_content += f"â° æ›´æ–°æ—¶é—´: {current_time}"
    
    # å‘é€åˆ°é£ä¹¦
    data = {
        "msg_type": "text",
        "content": {
            "text": text_content
        }
    }
    
    try:
        response = requests.post(FEISHU_WEBHOOK_URL, json=data, timeout=10)
        print(f"é£ä¹¦æ¨é€ç»“æœ: {response.status_code}")
        return response.status_code == 200
    except Exception as e:
        print(f"é£ä¹¦æ¨é€å¤±è´¥: {e}")
        return False

def main():
    print("å¼€å§‹è·å–ä»Šæ—¥çƒ­ç‚¹...")
    
    weibo_data = get_weibo_hot()
    zhihu_data = get_zhihu_hot()
    newrank_data = get_newrank_low_fans()
    
    # å‘é€åˆ°é£ä¹¦
    success = send_to_feishu(weibo_data, zhihu_data, newrank_data)
    
    if success:
        print("çƒ­ç‚¹æ¨é€å®Œæˆï¼")
    else:
        print("çƒ­ç‚¹æ¨é€å¤±è´¥ï¼")

if __name__ == '__main__':
    main()
