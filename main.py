import requests
import os
import json
from datetime import datetime
from urllib.parse import quote

FEISHU_WEBHOOK_URL = os.environ['FEISHU_WEBHOOK_URL']

def get_weibo_hot():
    """æŠ“å–å¾®åšçƒ­æœæ¦œ"""
    try:
        url = "https://weibo.com/ajax/side/hotSearch"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://weibo.com/',
        }
        response = requests.get(url, headers=headers, timeout=10)
        print(f"å¾®åšæ¥å£çŠ¶æ€ç : {response.status_code}")
        if response.status_code == 200:
            data = response.json()
            hot_list = []
            count = 0
            
            for item in data['data']['realtime']:
                if item.get('realpos') and item.get('realpos', 0) > 0 and item.get('flag') != 2:
                    search_word = quote(item['word'])
                    weibo_url = f"https://s.weibo.com/weibo?q={search_word}"
                    
                    hot_list.append({
                        'title': item['note'],
                        'url': weibo_url,
                        'rank': item['realpos']
                    })
                    count += 1
                    if count >= 10:
                        break
            
            hot_list.sort(key=lambda x: x['rank'])
            print(f"è¿‡æ»¤åçš„å¾®åšçƒ­æœæ•°é‡: {len(hot_list)}")
            return hot_list
        else:
            print(f"å¾®åšè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")
            return []
    except Exception as e:
        print(f"è·å–å¾®åšçƒ­æœå‡ºé”™: {e}")
        return []

def get_zhihu_hot():
    """æŠ“å–çŸ¥ä¹çƒ­æ¦œ"""
    try:
        url = "https://api.zhihu.com/topstory/hot-list?limit=10"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://www.zhihu.com/',
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        print(f"çŸ¥ä¹æ¥å£çŠ¶æ€ç : {response.status_code}")
        
        if response.status_code == 200:
            data = response.json()
            hot_list = []
            for item in data['data']:
                target = item.get('target', {})
                title = target.get('title', 'æ— æ ‡é¢˜')
                url = target.get('url', '')
                
                # ä¿®å¤çŸ¥ä¹é“¾æ¥
                if url and 'api.zhihu.com' in url:
                    if '/questions/' in url:
                        question_id = url.split('/questions/')[-1]
                        url = f"https://www.zhihu.com/question/{question_id}"
                    else:
                        url = "https://www.zhihu.com/hot"
                elif url and 'zhihu.com' not in url:
                    url = f"https://www.zhihu.com/question/{target.get('id', '')}"
                elif not url:
                    url = "https://www.zhihu.com/hot"
                
                hot_list.append({
                    'title': title,
                    'url': url
                })
            return hot_list
        else:
            print("çŸ¥ä¹æ¥å£å¤±è´¥ï¼Œè¿”å›æç¤ºä¿¡æ¯")
            return [{
                'title': 'âš ï¸ çŸ¥ä¹çƒ­æ¦œæš‚æ—¶æ— æ³•è·å–',
                'url': 'https://www.zhihu.com/hot'
            }]
            
    except Exception as e:
        print(f"è·å–çŸ¥ä¹çƒ­æ¦œå‡ºé”™: {e}")
        return [{
            'title': 'âš ï¸ çŸ¥ä¹çƒ­æ¦œè·å–å‡ºé”™',
            'url': 'https://www.zhihu.com/hot'
        }]

def get_newrank_low_fans():
    """æŠ“å–æ–°æ¦œä½ç²‰çˆ†æ–‡æ¦œTOP10 - æœ€ç»ˆé“¾æ¥ä¼˜åŒ–ç‰ˆ"""
    try:
        from playwright.sync_api import sync_playwright
        import os
        import re
        import time
        import urllib.parse
        
        def _is_valid_title(line, re_module):
            """åˆ¤æ–­ä¸€è¡Œæ–‡æœ¬æ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ–‡ç« æ ‡é¢˜"""
            # åŸºæœ¬é•¿åº¦æ£€æŸ¥
            if len(line) < 6 or len(line) > 120:
                return False
            
            # å¿…é¡»åŒ…å«ä¸­æ–‡
            if not any('\u4e00' <= char <= '\u9fff' for char in line):
                return False
            
            # æ’é™¤æ˜æ˜¾çš„éæ ‡é¢˜å†…å®¹
            exclude_patterns = [
                r'^ç²‰ä¸æ•°', r'^å‘å¸ƒäº', r'^é˜…è¯»æ•°', r'^ç‚¹èµæ•°', r'^è½¬å‘æ•°',
                r'^æ”¶è—', r'^æ›´å¤š', r'^ç™»å½•', r'^æ³¨å†Œ', r'^æ–°æ¦œ',
                r'^å¤´æ¡', r'^åŸ', r'^æƒ…æ„Ÿ', r'^æ–‡æ‘˜', r'^ç§‘æŠ€', r'^ç¾é£Ÿ',
                r'^ä¹æ´»', r'^èŒåœº',
                r'^\d+$', r'^[0-9.,wW\+]+$', r'^http', r'^Â©', r'^é¦–é¡µ'
            ]
            
            for pattern in exclude_patterns:
                if re_module.match(pattern, line):
                    return False
            
            # æ’é™¤åŒ…å«ä½œè€…ç‰¹å¾çš„è¡Œ
            author_indicators = ['ç²‰ä¸æ•°', 'å‘å¸ƒäº', 'æ·±è“ç”»ç”»', 'æ•…åœ’æŸ´æ‰‰', 'èŒ‰æ€¡è¯´', 'è€ç”°ç”µè„‘', 'èƒ¡è¨€å¨è¯­', 'å‚¬æ”¶åœˆ', 'é˜…äº«ä¹‹', 'çˆ±ç©¿è£™å­çš„é•¿æ¡‘']
            if any(indicator in line for indicator in author_indicators):
                return False
            
            # æ ‡é¢˜é€šå¸¸åŒ…å«æ ‡ç‚¹ç¬¦å·
            has_punctuation = any(char in line for char in ['ï¼š', 'ï¼', 'ï¼Ÿ', 'â€¦', 'ï¼Œ', 'ã€‚', '"', 'â€œ', 'â€', '.', '|', 'ã€', 'ã€', 'ã€Š', 'ã€‹', 'â€”â€”', 'ä¸'])
            
            # æ ‡é¢˜é€šå¸¸ä¸åŒ…å«ç»Ÿè®¡æ•°å­—æ¨¡å¼
            has_stats = bool(re_module.search(r'\d+[ä¸‡wW]', line)) or bool(re_module.search(r'\d+\.\d+[ä¸‡wW]?', line))
            
            # å®½æ¾æ¡ä»¶
            return (has_punctuation or len(line) > 8) and not has_stats
        
        def _extract_wechat_url_from_captcha(captcha_url):
            """ä»éªŒè¯é“¾æ¥ä¸­æå–çœŸå®çš„å¾®ä¿¡æ–‡ç« é“¾æ¥"""
            try:
                print(f"è§£æéªŒè¯é“¾æ¥: {captcha_url}")
                parsed_url = urllib.parse.urlparse(captcha_url)
                query_params = urllib.parse.parse_qs(parsed_url.query)
                
                if 'target_url' in query_params:
                    target_url = query_params['target_url'][0]
                    # URLè§£ç 
                    real_url = urllib.parse.unquote(target_url)
                    print(f"âœ… æå–åˆ°çœŸå®é“¾æ¥: {real_url}")
                    return real_url
                return captcha_url
            except Exception as e:
                print(f"è§£æéªŒè¯é“¾æ¥å¤±è´¥: {e}")
                return captcha_url
        
        def _get_article_url(row, page):
            """ä»è¡Œä¸­æå–çœŸå®çš„æ–‡ç« é“¾æ¥"""
            try:
                print("å¼€å§‹æå–æ–‡ç« é“¾æ¥...")
                
                # æ–¹æ³•1ï¼šç›´æ¥æŸ¥æ‰¾åŒ…å«æ–‡ç« æ•°æ®çš„å±æ€§
                # æ–°æ¦œé€šå¸¸åœ¨træˆ–tdä¸Šå­˜å‚¨æ–‡ç« æ•°æ®
                article_data = row.get_attribute('data-url') or row.get_attribute('data-link')
                if article_data:
                    print(f"âœ… ä»dataå±æ€§æ‰¾åˆ°é“¾æ¥: {article_data}")
                    if 'mp.weixin.qq.com' in article_data:
                        return _extract_wechat_url_from_captcha(article_data)
                    return article_data
                
                # æ–¹æ³•2ï¼šæŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„é“¾æ¥
                all_links = row.query_selector_all('a')
                article_candidates = []
                
                for link in all_links:
                    href = link.get_attribute('href')
                    text = link.inner_text().strip()
                    
                    if not href:
                        continue
                    
                    print(f"æ£€æŸ¥é“¾æ¥: æ–‡æœ¬='{text[:20]}...', href='{href}'")
                    
                    # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„æ–‡ç« é“¾æ¥
                    if any(pattern in href for pattern in ['/new/', '/detail/', 'mp.weixin.qq.com']):
                        article_candidates.append((href, text))
                
                # ä¼˜å…ˆå¤„ç†çœ‹èµ·æ¥åƒæ–‡ç« æ ‡é¢˜çš„é“¾æ¥
                for href, text in article_candidates:
                    if _is_valid_title(text, re) and len(text) > 10:
                        print(f"âœ… æ‰¾åˆ°æ ‡é¢˜é“¾æ¥: {href}")
                        return _resolve_article_url(href, page)
                
                # å¦‚æœæ²¡æœ‰æ ‡é¢˜é“¾æ¥ï¼Œå°è¯•ç¬¬ä¸€ä¸ªéä½œè€…é“¾æ¥
                for href, text in article_candidates:
                    if not any(keyword in text for keyword in ['ç²‰ä¸æ•°', 'å‘å¸ƒäº']):
                        print(f"âœ… å°è¯•éä½œè€…é“¾æ¥: {href}")
                        return _resolve_article_url(href, page)
                
                # æ–¹æ³•3ï¼šå¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œå°è¯•æ¨¡æ‹Ÿç‚¹å‡»æ ‡é¢˜åŒºåŸŸ
                print("å°è¯•é€šè¿‡ç‚¹å‡»è·å–é“¾æ¥...")
                title_cells = row.query_selector_all('td:nth-child(2), [class*="title"]')
                for cell in title_cells:
                    try:
                        # ä¿å­˜å½“å‰URL
                        original_url = page.url
                        
                        # ç‚¹å‡»æ ‡é¢˜å•å…ƒæ ¼
                        cell.click()
                        page.wait_for_timeout(3000)
                        
                        # æ£€æŸ¥æ˜¯å¦è·³è½¬
                        current_url = page.url
                        if current_url != original_url:
                            print(f"âœ… é€šè¿‡ç‚¹å‡»è·å–é“¾æ¥: {current_url}")
                            
                            # å¦‚æœæ˜¯å¾®ä¿¡é“¾æ¥ï¼Œç›´æ¥è¿”å›
                            if 'mp.weixin.qq.com' in current_url:
                                final_url = _extract_wechat_url_from_captcha(current_url)
                                # è¿”å›åŸé¡µé¢
                                page.goto(original_url, timeout=30000)
                                page.wait_for_timeout(2000)
                                return final_url
                            else:
                                # è¿”å›åŸé¡µé¢
                                page.goto(original_url, timeout=30000)
                                page.wait_for_timeout(2000)
                                return current_url
                        else:
                            # å¦‚æœæ²¡è·³è½¬ï¼Œè¿”å›åŸé¡µé¢
                            page.goto(original_url, timeout=30000)
                    except Exception as e:
                        print(f"ç‚¹å‡»å°è¯•å¤±è´¥: {e}")
                        continue
                
                print("âŒ æœªæ‰¾åˆ°æ–‡ç« é“¾æ¥")
                return "https://www.newrank.cn"
                
            except Exception as e:
                print(f"æå–é“¾æ¥å¤±è´¥: {e}")
                return "https://www.newrank.cn"
        
        def _resolve_article_url(newrank_url, page):
            """è§£ææ–°æ¦œæ–‡ç« é“¾æ¥è·å–çœŸå®å¾®ä¿¡æ–‡ç« åœ°å€"""
            try:
                print(f"å¼€å§‹è§£ææ–‡ç« é“¾æ¥: {newrank_url}")
                
                # ç¡®ä¿URLå®Œæ•´
                if not newrank_url.startswith('http'):
                    newrank_url = f"https://www.newrank.cn{newrank_url}"
                
                print(f"è®¿é—®æ–°æ¦œæ–‡ç« é¡µ: {newrank_url}")
                
                # åœ¨æ–°æ ‡ç­¾é¡µä¸­æ‰“å¼€æ–‡ç« é¡µé¢
                new_page = page.context.new_page()
                new_page.goto(newrank_url, timeout=30000)
                new_page.wait_for_timeout(5000)
                
                # è·å–å½“å‰URL
                current_url = new_page.url
                print(f"è§£æåURL: {current_url}")
                
                # å¦‚æœæ˜¯å¾®ä¿¡éªŒè¯é“¾æ¥ï¼Œæå–çœŸå®URL
                if 'wappoc_appmsgcaptcha' in current_url:
                    real_url = _extract_wechat_url_from_captcha(current_url)
                    new_page.close()
                    return real_url
                
                # å¦‚æœæ˜¯ç›´æ¥å¾®ä¿¡æ–‡ç« é“¾æ¥
                if 'mp.weixin.qq.com/s?' in current_url:
                    print(f"âœ… æ‰¾åˆ°ç›´æ¥å¾®ä¿¡æ–‡ç« é“¾æ¥: {current_url}")
                    new_page.close()
                    return current_url
                
                # æŸ¥æ‰¾å¾®ä¿¡iframe
                wechat_iframe = new_page.query_selector('iframe[src*="mp.weixin.qq.com"]')
                if wechat_iframe:
                    iframe_src = wechat_iframe.get_attribute('src')
                    print(f"âœ… æ‰¾åˆ°å¾®ä¿¡iframe: {iframe_src}")
                    new_page.close()
                    return iframe_src
                
                # æŸ¥æ‰¾è·³è½¬æŒ‰é’®æˆ–é“¾æ¥
                wechat_links = new_page.query_selector_all('a[href*="mp.weixin.qq.com"]')
                for link in wechat_links:
                    href = link.get_attribute('href')
                    if href and ('/s?' in href or 'wappoc_appmsgcaptcha' in href):
                        print(f"âœ… æ‰¾åˆ°å¾®ä¿¡è·³è½¬é“¾æ¥: {href}")
                        new_page.close()
                        return _extract_wechat_url_from_captcha(href)
                
                print(f"âŒ æœªæ‰¾åˆ°å¾®ä¿¡é“¾æ¥ï¼Œè¿”å›: {current_url}")
                new_page.close()
                return current_url
                    
            except Exception as e:
                print(f"è§£ææ–‡ç« é“¾æ¥å¤±è´¥: {e}")
                try:
                    new_page.close()
                except:
                    pass
                return "https://www.newrank.cn"
        
        print("å¼€å§‹æŠ“å–æ–°æ¦œä½ç²‰çˆ†æ–‡æ¦œ...")
        newrank_list = []
        
        # ä»ç¯å¢ƒå˜é‡è·å–Cookie
        newrank_cookie = os.environ.get('NEWRANK_COOKIE', '')
        
        if not newrank_cookie:
            return [{
                'title': 'âš ï¸ æœªè®¾ç½®æ–°æ¦œCookie',
                'url': 'https://www.newrank.cn/hotInfo?platform=GZH&rankType=3'
            }]
        
        print("ä½¿ç”¨æœ€ç»ˆä¼˜åŒ–æ–¹æ³•æå–æ–‡ç« æ ‡é¢˜å’Œé“¾æ¥...")
        
        with sync_playwright() as p:
            # å¯åŠ¨æµè§ˆå™¨
            browser = p.chromium.launch(headless=True)
            context = browser.new_context()
            page = context.new_page()
            
            # è®¾ç½®æ›´å¤§çš„è§†çª—
            page.set_viewport_size({"width": 1920, "height": 1080})
            
            # è®¾ç½®Cookie
            print("è®¾ç½®ç™»å½•Cookie...")
            cookies = []
            cookie_pairs = newrank_cookie.split(';')
            
            for cookie_str in cookie_pairs:
                cookie_str = cookie_str.strip()
                if '=' in cookie_str:
                    name, value = cookie_str.split('=', 1)
                    cookies.append({
                        'name': name.strip(),
                        'value': value.strip(),
                        'domain': '.newrank.cn',
                        'path': '/'
                    })
            
            # å…ˆè®¿é—®é¦–é¡µè®¾ç½®Cookie
            page.goto('https://www.newrank.cn/', timeout=30000)
            context.add_cookies(cookies)
            
            # è®¿é—®ç›®æ ‡é¡µé¢
            timestamp = int(time.time())
            target_url = f"https://www.newrank.cn/hotInfo?platform=GZH&rankType=3&t={timestamp}"
            print("è®¿é—®ä½ç²‰çˆ†æ–‡æ¦œé¡µé¢...")
            page.goto(target_url, timeout=60000)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            print("ç­‰å¾…æ¦œå•æ•°æ®åŠ è½½...")
            page.wait_for_timeout(15000)
            
            # æ–¹æ³•ï¼šç›´æ¥ä»è¡¨æ ¼ä¸­æå–å‰10æ¡
            print("ä»è¡¨æ ¼ä¸­æå–å‰10æ¡...")
            
            # æŸ¥æ‰¾è¡¨æ ¼
            table_selectors = ['.ant-table-tbody', 'table tbody', '[class*="table"]']
            table_body = None
            for selector in table_selectors:
                table_body = page.query_selector(selector)
                if table_body:
                    table_text = table_body.inner_text()
                    if any(keyword in table_text for keyword in ['é˜…è¯»æ•°', 'ç‚¹èµæ•°', 'è½¬å‘æ•°']):
                        print(f"æ‰¾åˆ°æ¦œå•è¡¨æ ¼: {selector}")
                        break
                    else:
                        table_body = None
            
            if table_body:
                rows = table_body.query_selector_all('tr')
                print(f"è¡¨æ ¼ä¸­æœ‰ {len(rows)} è¡Œ")
                
                # å¤„ç†ç¬¬3-12è¡Œï¼ˆå®é™…çš„æ•°æ®è¡Œï¼‰
                for i in range(2, min(12, len(rows))):
                    row = rows[i]
                    
                    try:
                        # è·å–æ•´è¡Œæ–‡æœ¬
                        row_text = row.inner_text()
                        lines = [line.strip() for line in row_text.split('\n') if line.strip()]
                        
                        print(f"ç¬¬{i+1}è¡Œå†…å®¹: {lines}")
                        
                        # åœ¨è¡Œä¸­å¯»æ‰¾æ ‡é¢˜ï¼šé€šå¸¸æ˜¯ç¬¬äºŒä¸ªå…ƒç´ ï¼ˆç´¢å¼•1ï¼‰
                        if len(lines) > 1:
                            title = lines[1]
                            
                            if _is_valid_title(title, re):
                                # è·å–çœŸå®çš„æ–‡ç« é“¾æ¥
                                print(f"æ­£åœ¨ä¸ºæ ‡é¢˜ '{title}' æå–é“¾æ¥...")
                                article_url = _get_article_url(row, page)
                                
                                newrank_list.append({
                                    'title': title,
                                    'url': article_url
                                })
                                print(f"âœ… æå–ç¬¬{len(newrank_list)}æ¡: {title}")
                                print(f"   æœ€ç»ˆé“¾æ¥: {article_url}")
                            else:
                                print(f"âŒ æ ‡é¢˜éªŒè¯å¤±è´¥: {title}")
                                
                    except Exception as e:
                        print(f"å¤„ç†ç¬¬{i+1}è¡Œæ—¶å‡ºé”™: {e}")
                        continue
            
            browser.close()
        
        print(f"æˆåŠŸè·å–æ–°æ¦œæ•°æ® {len(newrank_list)} æ¡")
        return newrank_list[:10]
        
    except Exception as e:
        print(f"è·å–æ–°æ¦œä½ç²‰çˆ†æ–‡æ¦œå‡ºé”™: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return [{
            'title': 'âš ï¸ æ–°æ¦œä½ç²‰çˆ†æ–‡æ¦œè·å–å¤±è´¥',
            'url': 'https://www.newrank.cn/hotInfo?platform=GZH&rankType=3'
        }]
        
def send_to_feishu(weibo_data, zhihu_data, newrank_data):
    """å‘é€æ¶ˆæ¯åˆ°é£ä¹¦"""
    text_content = "ğŸŒ æ¯æ—¥çƒ­ç‚¹é€Ÿé€’\n\n"
    
    # å¾®åšéƒ¨åˆ†
    if weibo_data and len(weibo_data) > 0:
        text_content += "ã€ğŸ”¥ å¾®åšçƒ­æœ TOP 10ã€‘â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n"
        for i, item in enumerate(weibo_data, 1):
            text_content += f"{i}. {item['title']}\n   ğŸ”— {item['url']}\n"
        text_content += "\n"
    
    # çŸ¥ä¹éƒ¨åˆ†
    if zhihu_data and len(zhihu_data) > 0:
        text_content += "ã€ğŸ“š çŸ¥ä¹çƒ­æ¦œ TOP 30ã€‘â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n"
        for i, item in enumerate(zhihu_data, 1):
            text_content += f"{i}. {item['title']}\n"
            if 'zhihu.com' in item['url']:
                text_content += f"   ğŸ”— {item['url']}\n"
        text_content += "\n"
    
    # æ–°æ¦œä½ç²‰çˆ†æ–‡æ¦œéƒ¨åˆ†
    if newrank_data and len(newrank_data) > 0:
        text_content += "ã€ğŸ’¥ æ–°æ¦œä½ç²‰çˆ†æ–‡æ¦œ TOP 10ã€‘â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n"
        for i, item in enumerate(newrank_data, 1):
            text_content += f"{i}. {item['title']}\n"
            if 'newrank.cn' in item['url']:
                text_content += f"   ğŸ”— {item['url']}\n"
        text_content += "\n"
    
    # æ·»åŠ æ—¶é—´æˆ³
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    text_content += f"â° æ›´æ–°æ—¶é—´: {current_time}"
    
    # å‘é€åˆ°é£ä¹¦
    data = {
        "msg_type": "text",
        "content": {
            "text": text_content
        }
    }
    
    try:
        response = requests.post(FEISHU_WEBHOOK_URL, json=data, timeout=10)
        print(f"é£ä¹¦æ¨é€ç»“æœ: {response.status_code}")
        return response.status_code == 200
    except Exception as e:
        print(f"é£ä¹¦æ¨é€å¤±è´¥: {e}")
        return False

def main():
    print("å¼€å§‹è·å–ä»Šæ—¥çƒ­ç‚¹...")
    
    weibo_data = get_weibo_hot()
    zhihu_data = get_zhihu_hot()
    newrank_data = get_newrank_low_fans()
    
    # å‘é€åˆ°é£ä¹¦
    success = send_to_feishu(weibo_data, zhihu_data, newrank_data)
    
    if success:
        print("çƒ­ç‚¹æ¨é€å®Œæˆï¼")
    else:
        print("çƒ­ç‚¹æ¨é€å¤±è´¥ï¼")

if __name__ == '__main__':
    main()
